{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ml_utils' from '../scripts/ml_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import igraph as ig\n",
    "import sys, time, re\n",
    "from random import randint\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('../scripts')\n",
    "%matplotlib inline \n",
    "folder = '../data/'\n",
    "\n",
    "import ml_utils as utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyrtranslit\n",
    "\n",
    "def read_edges(f_name):\n",
    "    print(f_name)\n",
    "    g = ig.Graph.Read_Ncol(f_name, names=True, directed=False)\n",
    "    ig.summary(g)\n",
    "    return g\n",
    "\n",
    "def enrich_vk_graph(g):\n",
    "    data_dict = dict()\n",
    "    pat = re.compile(\"(\\d+),(.*),(.*),(.*)\")\n",
    "    pat_word = re.compile('[^a-zA-Zа-яА-Я\\d\\s]+')\n",
    "    \n",
    "    g.vs['fname'] = ''\n",
    "    g.vs['uid'] = None\n",
    "    \n",
    "    with open(folder + 'vk_personal2.csv', 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                uid, uname, name1, name2 = pat.match(line).groups()\n",
    "                name1 = re.sub(pat_word, '', name1).strip().lower()\n",
    "                name2 = re.sub(pat_word, '', name2).strip().lower()\n",
    "                data_dict[uid] = (uname, name1 + ' ' + name2)\n",
    "            except AttributeError:\n",
    "                print(line)\n",
    "    for v in g.vs:\n",
    "        uid = v['name']\n",
    "        uname, fname = data_dict[uid]\n",
    "        v['name'] = uname\n",
    "        v['uid'] = int(uid)\n",
    "        v['fname'] = cyrtranslit.to_latin(fname, 'ru').replace(\"'\", '')\n",
    "\n",
    "def enrich_insta_graph(g):\n",
    "    data_dict = dict()\n",
    "    pat = re.compile(\"(\\d+),(.*),(.*)\")\n",
    "    pat_word = re.compile('[^a-zA-Zа-яА-Я\\d\\s]+')\n",
    "    \n",
    "    g.vs['fname'] = ''\n",
    "    g.vs['uid'] = None\n",
    "    \n",
    "    with open(folder + 'inst_personal.csv', 'r') as f:\n",
    "        for line in f:\n",
    "            uid, uname, fname = pat.match(line).groups()\n",
    "            fname = re.sub(pat_word, '', fname).strip().lower()\n",
    "            data_dict[uid] = (uname, fname)\n",
    "\n",
    "    for v in g.vs:\n",
    "        uid = v['name']\n",
    "        uname, fname = data_dict[uid]\n",
    "        v['name'] = uname\n",
    "        v['uid'] = int(uid)\n",
    "        v['fname'] = cyrtranslit.to_latin(fname, 'ru').replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_g = read_edges(folder + 'inst_lid_rid.csv')\n",
    "enrich_insta_graph(inst_g)\n",
    "\n",
    "vk_g = read_edges(folder + 'vk_lid_rid.csv')\n",
    "enrich_vk_graph(vk_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk_g.write_pickle(fname=os.path.join(folder, 'vk.pickle'))\n",
    "inst_g.write_pickle(fname=os.path.join(folder, 'inst.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk_g = ig.Graph.Read_Pickle(os.path.join(folder, 'vk.pickle'))\n",
    "inst_g = ig.Graph.Read_Pickle(os.path.join(folder, 'inst.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matches to lid_rid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../matches/repeat/091/matches_s_01_th_091_t_10-12_13:19.pickle\n",
      "matches len 4765\n",
      "(78897392, 608311198)\n",
      "(20158885, 482179932)\n",
      "(12852529, 294223340)\n",
      "(239645190, 53256746)\n",
      "(134652490, 4145166609)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9565582371458552, 0.22797979292752463)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold=91\n",
    "is_repeat = True\n",
    "matches_file_name = 'matches_s_01_th_091_t_10-12_13:19.pickle'\n",
    "\n",
    "matches = utils.read_matches(matches_file_name, threshold, is_repeat)\n",
    "utils.precision_recall(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_dist(g):\n",
    "    l = []\n",
    "    for v in g.vs:\n",
    "        l.append(v.degree())\n",
    "    plt.hist(l, bins=30)\n",
    "    plt.title('Degree distribution')\n",
    "    plt.show()\n",
    "    \n",
    "def double_deg_dist(g):\n",
    "    l = []\n",
    "    for v in g.vs:\n",
    "        for vn in v.neighbors():\n",
    "            l.append(vn.degree())\n",
    "    plt.hist(l, bins=30)\n",
    "    plt.title('Double Degree distribution')\n",
    "    plt.show()\n",
    "    \n",
    "def double_deg_dist_mean(g):\n",
    "    l = []\n",
    "    for v in g.vs:\n",
    "        s = []\n",
    "        for vn in v.neighbors():\n",
    "            s.append(vn.degree())\n",
    "        l.append(sum(s)/len(s))\n",
    "    plt.hist(l, bins=30)\n",
    "    plt.title('Mean of double degree distribution')\n",
    "    plt.show()\n",
    "    \n",
    "def double_deg_dist_median(g):\n",
    "    l = []\n",
    "    for v in g.vs:\n",
    "        s = []\n",
    "        for vn in v.neighbors():\n",
    "            s.append(vn.degree())\n",
    "        l.append(sorted(s)[len(s)//2])\n",
    "    plt.hist(l, bins=30)\n",
    "    plt.title('Median of double degree distribution')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_dist(g):\n",
    "    degree_dist(g)\n",
    "    double_deg_dist_mean(g)\n",
    "    double_deg_dist(g)\n",
    "    double_deg_dist_median(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_dist(vk_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_dist(inst_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose name similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "df = utils.read_combine_df(from_raw = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex(name, len=4):\n",
    "    \"\"\" soundex module conforming to Knuth's algorithm\n",
    "        implementation 2000-12-24 by Gregory Jorgensen\n",
    "        public domain\n",
    "    \"\"\"\n",
    "\n",
    "    # digits holds the soundex values for the alphabet\n",
    "    digits = '01230120022455012623010202'\n",
    "    sndx = ''\n",
    "    fc = ''\n",
    "\n",
    "    # translate alpha chars in name to soundex digits\n",
    "    for c in name.upper():\n",
    "        if c.isalpha():\n",
    "            if not fc: fc = c   # remember first letter\n",
    "            d = digits[ord(c)-ord('A')]\n",
    "            # duplicate consecutive soundex digits are skipped\n",
    "            if not sndx or (d != sndx[-1]):\n",
    "                sndx += d\n",
    "\n",
    "    # replace first digit with first alpha character\n",
    "    sndx = fc + sndx[1:]\n",
    "\n",
    "    # remove all 0s from the soundex code\n",
    "    sndx = sndx.replace('0','')\n",
    "\n",
    "    # return soundex code padded to len characters\n",
    "    return (sndx + (len * '0'))[:len]\n",
    "\n",
    "def soundex_sim(a,b):\n",
    "    a = soundex(a)\n",
    "    b = soundex(b)\n",
    "    return fuzz.token_sort_ratio(a,b) / 100\n",
    "\n",
    "soundex('ildar nurgaliev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import Levenshtein as lev\n",
    "from difflib import SequenceMatcher\n",
    "from time import time\n",
    "\n",
    "a = 'Ildar Nurgaliev'\n",
    "b = 'Nurgaliev Ildar'\n",
    "\n",
    "s = time()\n",
    "print(fuzz.ratio(a,b) / 100)\n",
    "e = time()\n",
    "print('\\tfuzz.ratio', e-s)\n",
    "\n",
    "s = time()\n",
    "print(fuzz.partial_ratio(a,b) / 100)\n",
    "e = time()\n",
    "print('\\tpartial_ratio', e-s)\n",
    "\n",
    "s = time()\n",
    "print(fuzz.token_sort_ratio(a,b) / 100)\n",
    "e = time()\n",
    "print('\\ttoken_sort_ratio', e-s)\n",
    "\n",
    "s = time()\n",
    "print(fuzz.token_set_ratio(a,b) / 100)\n",
    "e = time()\n",
    "print('\\ttoken_set_ratio', e-s)\n",
    "\n",
    "s = time()\n",
    "print(lev.ratio(a,b))\n",
    "e = time()\n",
    "print('\\tlev.ratio', e-s)\n",
    "\n",
    "s = time()\n",
    "seq = SequenceMatcher(None, a, b)\n",
    "print(seq.ratio())\n",
    "e = time()\n",
    "print('\\tSequenceMatcher', e-s)\n",
    "\n",
    "s = time()\n",
    "print(soundex_sim(a,b))\n",
    "e = time()\n",
    "print('\\tsound', e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_name_sim_fs():\n",
    "    for row in df[['uname', 'name_inst', 'name_vk']].values[:150]:\n",
    "        \n",
    "        a,b = row[1:]\n",
    "        s1 = lev.ratio(a,b)\n",
    "        s2 = fuzz.token_sort_ratio(a,b) / 100\n",
    "        s3 = fuzz.token_set_ratio(a,b) / 100\n",
    "        c = s1 < s2\n",
    "        if c:\n",
    "            print('%s \\t %s : %s' % (row[0], row[1], row[2]))\n",
    "            print('!' if c else '','%.3f %.3f %.3f' % (s1, s2, s3))\n",
    "        print()\n",
    "\n",
    "test_name_sim_fs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_permutation_test(pooled,sizeZ,sizeY,delta):\n",
    "    np.random.shuffle(pooled)\n",
    "    starZ = pooled[:sizeZ]\n",
    "    starY = pooled[-sizeY:]\n",
    "    return starZ.mean() - starY.mean()\n",
    "\n",
    "def bootstrap_test(z,y):\n",
    "    numSamples = 10000\n",
    "    \n",
    "    z = np.array(z)\n",
    "    y = np.array(y)\n",
    "\n",
    "    pooled = np.hstack([z,y])\n",
    "    delta = z.mean() - y.mean()\n",
    "    estimates = np.array(list(map(lambda x: run_permutation_test(pooled,z.size,y.size,delta),range(numSamples))))\n",
    "    diffCount = len(np.where(estimates <= delta)[0])\n",
    "    hat_asl_perm = 1.0 - (float(diffCount)/float(numSamples))\n",
    "    return hat_asl_perm\n",
    "\n",
    "\n",
    "bootstrap_test([94,197,16,38,99,141,23], [52,104,146,10,51,30,40,27,46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(vl, bins=31, size=50):\n",
    "    feature_set = [0 for i in range(2 * bins)]\n",
    "    _1hop = [v.degree() for v in vl.neighbors()]\n",
    "    for h in _1hop:\n",
    "        if h < bins * size:\n",
    "            feature_set[int(h / size)] += 1\n",
    "#     _2hop = []\n",
    "#     for vs in vl.neighbors():\n",
    "#         _2hop += [v.degree() for v in vs.neighbors()]\n",
    "#     for h in _2hop:\n",
    "#         if h < bins * size:\n",
    "#             feature_set[bins + int(h / size)] += 1\n",
    "    return feature_set\n",
    "\n",
    "a = feature(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools  as it\n",
    "from scipy.stats import ks_2samp\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "def dist_sim(vr, vl):\n",
    "    sl = [v.degree() for v in vl.neighbors()]\n",
    "    sr = [v.degree() for v in vr.neighbors()]\n",
    "    return ks_2samp(sl, sr)\n",
    "\n",
    "\n",
    "def dist_sim2(vl, vr):\n",
    "#     sl = []\n",
    "#     for vs in vl.neighbors():\n",
    "#         sl += [v.degree() for v in vs.neighbors()]\n",
    "        \n",
    "#     sr = []\n",
    "#     for vs in vl.neighbors():\n",
    "#         sr += [v.degree() for v in vs.neighbors()]\n",
    "#     sl = [v.degree() for v in vl.neighbors()]\n",
    "#     sr = [v.degree() for v in vr.neighbors()]\n",
    "    bins=11\n",
    "    size=50\n",
    "    sl = feature(vl, bins, size)\n",
    "    sr = feature(vr, bins, size)\n",
    "    return bootstrap_test(sl, sr)\n",
    "\n",
    "def neigbor_deg_dist(lg, rg):\n",
    "    seq = SequenceMatcher()\n",
    "#     for vl, vr in zip(it.islice(lg.vs, 40,50), it.islice(rg.vs, 10,30)):\n",
    "    for vl in it.islice(lg.vs, 41, 42):\n",
    "        vrt = rg.vs.find(name = vl['name'])\n",
    "        \n",
    "        for vr in [vrt] + vrt.neighbors():\n",
    "            seq.set_seqs(vl['fname'], vr['fname'])\n",
    "            t2 = dist_sim2(vl, vr)\n",
    "            s = []\n",
    "            for vn in vl.neighbors():\n",
    "                s.append(vn.degree())\n",
    "            f, axarr = plt.subplots(nrows = 1, ncols=2)\n",
    "            axarr[0].hist(s, bins=30)\n",
    "            axarr[0].set_title('%s %d\\n %s %f\\n %f' % (vl['name'], vl.degree(), vl['fname'], seq.ratio(), t2))\n",
    "            for vn in vr.neighbors():\n",
    "                s.append(vn.degree())\n",
    "            axarr[1].hist(s, bins=30)\n",
    "\n",
    "            plt.title('%s %d\\n %s' % (vr['name'], vr.degree(), vr['fname']))\n",
    "            plt.show()\n",
    "            print(dist_sim(vl, vr))\n",
    "    \n",
    "neigbor_deg_dist(vk_g, inst_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

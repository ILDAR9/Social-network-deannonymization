{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ml_utils' from '../scripts/ml_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, time, re\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "from importlib import reload\n",
    "%matplotlib inline\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "import  Levenshtein\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import ml_utils as utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME='UIL'\n",
    "sc = None\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/home/ildar/anaconda3/envs/ds/bin/python'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/home/ildar/anaconda3/envs/ds/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareCtx():\n",
    "    global sc\n",
    "    if sc:\n",
    "        sc.stop()\n",
    "    \n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    conf = (SparkConf().setAppName(APP_NAME).setMaster('local[*]') #.setMaster('spark://bmi:7077')\n",
    "            .set('spark.executor.memory', '2G')\n",
    "            .set('spark.driver.memory', '12G')\n",
    "#             .set('spark.driver.maxResultSize', '12G')\n",
    "           )\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "        \n",
    "\n",
    "prepareCtx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "NUM_SAMPLES=100000\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg, rg = G1, G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.52 s, sys: 348 ms, total: 6.87 s\n",
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def map_spread_mark(seed):\n",
    "    lnode, rnode = seed[:2]\n",
    "\n",
    "    seeds_collect = {}\n",
    "    for l_neighbor in lg.neighbors(lnode):\n",
    "        for r_neighbor in rg.neighbors(rnode):\n",
    "            candidate = (l_neighbor, r_neighbor)\n",
    "            seeds_collect[candidate] = seeds_collect.get(candidate, 0) + 1\n",
    "    return seeds_collect\n",
    "\n",
    "seeds = matches\n",
    "res2 = list(map(map_spread_mark, seeds))\n",
    "del res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 728 ms, total: 13.3 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        self.lg = G1\n",
    "        self.rg = G2\n",
    "        self.seeds = matches\n",
    "        self.seeds_collect={}\n",
    "\n",
    "    def to_str(self, ln ,rn): return '%d|%d' % (ln, rn)\n",
    "\n",
    "    def map_spread_mark(self, seeds):\n",
    "        lnode, rnode = seed[:2]\n",
    "        \n",
    "        for l_neighbor in self.lg.neighbors(lnode):\n",
    "            for r_neighbor in self.rg.neighbors(rnode):\n",
    "                candidate = (l_neighbor, r_neighbor)\n",
    "                self.seeds_collect[candidate] =self.seeds_collect.get(candidate, 0) + 1\n",
    "#         return seeds_collect\n",
    "\n",
    "    def start_parallel(self):\n",
    "       \n",
    "        distSeeds = sc.parallelize(self.seeds)\n",
    "        res =  distSeeds.map(map_spread_mark).collect()\n",
    "        return res\n",
    "        \n",
    "a = A()\n",
    "\n",
    "res = a.start_parallel()\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.seeds_collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __spread_mark(lnode, rnode):\n",
    "    # add one mark to all neighboring pairs of [i,j]\n",
    "    if not seeds_collect:\n",
    "        seeds_collect = {}\n",
    "        old_marks = {}\n",
    "        is_from_spread_marks=False\n",
    "    else:\n",
    "        is_from_spread_marks=True\n",
    "    for l_neighbor in self.lg.neighbors(lnode):\n",
    "        for r_neighbor in self.rg.neighbors(rnode):\n",
    "            ID_str = self.__decide_seed(l_neighbor, r_neighbor)\n",
    "            if not ID_str:\n",
    "                continue\n",
    "\n",
    "            val = self.score_map.get(ID_str)\n",
    "            if not val:\n",
    "         distSeeds       self.score_map[ID_str] = 1\n",
    "                continue\n",
    "            if ID_str not in old_marks:\n",
    "                old_msarks[ID_str] = val\n",
    "            self.score_map[ID_str] += 1\n",
    "            seeds_collect[(l_neighbor, r_neighbor)] = val+1\n",
    "    if is_from_spread_marks:\n",
    "        return\n",
    "    for seed, marks_count in seeds_collect.items():\n",
    "        ID_str = self.to_str(*seed)\n",
    "        m = (seed[0], seed[1], old_marks[ID_str])\n",
    "        self.inactive_pairs.discard(m)\n",
    "        self.inactive_pairs.add((seed[0], seed[1], marks_count))\n",
    "\n",
    "def __spread_marks():\n",
    "    # for all pairs[i, j] of A do\n",
    "    print('start __spread_marks')\n",
    "    seeds_collect = {}\n",
    "    old_marks = {}\n",
    "    for seed in tqdm(self.seeds):\n",
    "        self.used.add(self.to_str(*seed))\n",
    "        self.__spread_mark(*seed, seeds_collect = seeds_collect, old_marks = old_marks)\n",
    "s\n",
    "    for seed, marks_count in tqdm(seeds_collect.items()):\n",
    "        ID_str = self.to_str(*seed)\n",
    "        m = (seed[0], seed[1], old_marks[ID_str])\n",
    "        self.inactive_pairs.discard(m)\n",
    "        self.inactive_pairs.add((seed[0], seed[1], marks_count))\n",
    "\n",
    "    # A <- None\n",
    "    self.seeds.clear()\n",
    "    print(\"Seed are expanded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1 has all fname True\n",
      "G2 has all fname False\n",
      "CPU times: user 376 ms, sys: 40 ms, total: 416 ms\n",
      "Wall time: 414 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reload(utils)\n",
    "g1_fname = 'vk_lid_rid.csv'\n",
    "g2_fname = 'inst_lid_rid.csv'\n",
    "G1, G2 = utils.read_gs(g1_fname, g2_fname, from_raw = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=70\n",
    "folder_type = 0\n",
    "matches_file_name = 'matches_s_10_th_070_t_10-17_17:04:48.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=91\n",
    "folder_type = 0\n",
    "matches_file_name = 'matches_s_03_th_091_t_10-19_23:31:15.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=81\n",
    "folder_type = 0\n",
    "matches_file_name = 'matches_s_03_th_081_t_10-20_00:10:59.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me/ildar/projects/pycharm/social_network_revealing/graphmatching/matches/no_repeat/091/matches_s_03_th_091_t_10-19_23:31:15.pickle\n",
      "matches len 4124\n",
      "(78897392, 608311198)\n",
      "True 3980 False 144\n",
      "(0.965082444228904, 0.16140151668761912)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4121, 1113, 'total', 5234)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(utils)\n",
    "\n",
    "matches, seeds_features = utils.read_matches(matches_file_name, threshold, folder_type, with_train=True)\n",
    "print(utils.precision_recall(matches))\n",
    "\n",
    "fets_total = len(set(seeds_features))\n",
    "fs = len(set(seeds_features) - set(matches))\n",
    "fets_total - fs, fs, 'total', fets_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_data_matches_f86_1hop_ratio\n",
    "- train_data_matches_f85_1hop\n",
    "- train_data_matches_f86_1-2hop\n",
    "- train_data_matches_f85_th91\n",
    "- train_data_matches_f85_th81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "utils.gen_train_data(matches, G1, G2, save_to = 'train_data_matches_f85_th81', threads = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = utils.load_train_data('train_data_matches_f85_th81') #train_data_matches_f86_1-2hop\n",
    "print(len(features[0]))\n",
    "len(features), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate true mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.generate_true_mapping(from_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.read_combine_df(from_raw=False, merge_how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.f_set1s = dict(pickle.load(open(os.path.join(utils.folder_gen, 'features_G1.pickle'), \"rb\")))\n",
    "utils.f_set2s = dict(pickle.load(open(os.path.join(utils.folder_gen, 'features_G2.pickle'), \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_use = features\n",
    "labels_use = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#      features_use, labels_use, test_size=0.3, random_state=42)\n",
    "x_train, y_train = features, labels\n",
    "\n",
    "len(x_train[0]), len(features_use[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 400, criterion ='gini',\n",
    "                        bootstrap = True, max_features = 'auto' ,n_jobs=4)\n",
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100)\n",
    "gbc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = forest.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b',\n",
    "label='GS, AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(forest, open(os.path.join(utils.folder_gen, 'forest.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest2 = pickle.load(open(os.path.join(utils.folder_gen, 'forest.pickle'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try keras ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import mean_squared_error\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM,Activation,Dense, Lambda, Input, Concatenate, Add, Reshape, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mean_squared_error\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.engine.topology import Layer\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import multiply\n",
    "from keras import backend as K\n",
    "from keras.constraints import non_neg\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Формирование модели\n",
    "model_number = 0\n",
    "best_weights_filepath = './best_weights.hdf5'\n",
    "log_path = './logs_nn/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin=42\n",
    "def get_model(x_train,\n",
    "              y_train,\n",
    "              model_number=0,\n",
    "              best_weights_filepath=best_weights_filepath,\n",
    "              log_path='./logs_nn/'):\n",
    "    '''hyperparams'''\n",
    "#     learning_rate = 1e-3\n",
    "#     reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "#     optimiser = Adam(lr=learning_rate)\n",
    "    dense_activation = 'relu'\n",
    "    dense_1layer_nn_count_coef = 1\n",
    "    dense_2layer_nn_count_coef = 1\n",
    "    \n",
    "    out_len = 1\n",
    "    feature_amount = len(x_train[0])\n",
    "    others_count = feature_amount - bin*2\n",
    "    in_shape = (feature_amount, )\n",
    "    \n",
    "    ''' Input '''\n",
    "    inputs = Input(shape=in_shape, name=\"in_data\")\n",
    "    inputs_transformed = BatchNormalization()(inputs)\n",
    "    \n",
    "    ''' Slices '''\n",
    "    input_bins = [None]*bin\n",
    "    for i in range(bin):\n",
    "        slice_0 = Lambda(lambda x: x[:,i:i+1],\n",
    "            output_shape=lambda in_shape: (in_shape[0], 1))(inputs_transformed)\n",
    "        slice_1 = Lambda(lambda x: x[:,bin+i:bin+i+1],\n",
    "            output_shape=lambda in_shape: (in_shape[0], 1))(inputs_transformed)\n",
    "        input_bins[i] = keras.layers.concatenate([slice_0, slice_1])\n",
    "\n",
    "    slice_others = Lambda(lambda x: x[:,-others_count:],\n",
    "            output_shape=lambda in_shape: (in_shape[0], feature_amount - bin*2))(inputs_transformed)\n",
    "\n",
    "    ''' Dense for input_bins '''\n",
    "    pair_denses = [None]*bin\n",
    "    for i in range(bin):\n",
    "        input_bins[i]\n",
    "        pair_denses[i] = Dense(units = 1, activation = dense_activation)(input_bins[i])\n",
    "    \n",
    "    ''' Dense others '''\n",
    "    dense_others = Dense(units = others_count, activation = dense_activation)(slice_others)\n",
    "    \n",
    "    ''' Combine Denses '''\n",
    "    combine_pairs_and_others = keras.layers.concatenate(pair_denses + [dense_others])\n",
    "    dense_nn_count = bin + others_count\n",
    "    dropout = Dropout(0.2)(combine_pairs_and_others)\n",
    "    dense_model = Dense(units = dense_nn_count, activation = dense_activation)(dropout)    \n",
    "    \n",
    "    ''' Final Denses '''\n",
    "    dropout = Dropout(0.1)(dense_model)\n",
    "    dense_nn_count = bin // 2 + others_count // 2\n",
    "    dense_model = Dense(units = dense_nn_count, activation = dense_activation)(dense_model)\n",
    "    output = Dense(units = out_len, activation = 'sigmoid', name='out_states')(dense_model)   \n",
    "    \n",
    "    model = Model(inputs, output)\n",
    "    \n",
    "    tb_callback = TensorBoard(log_dir=log_path + '/', histogram_freq=0, write_graph=True,\n",
    "                              write_images=True)\n",
    "    bm_callback = ModelCheckpoint(best_weights_filepath, monitor='acc', verbose=1, save_best_only=True,\n",
    "                                  mode='auto')\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop', #'sgd'\n",
    "                  metrics=['accuracy'])\n",
    "    return [model, [bm_callback, tb_callback]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs_nn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number += 1\n",
    "[model, callbacks] = get_model(x_train, y_train, log_path = log_path+str(model_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 1800\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs,\n",
    "#       validation_data=(x_test, y_test),\n",
    "      initial_epoch = 724,\n",
    "      callbacks = callbacks,\n",
    "      verbose = 1\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "model.load_weights(best_weights_filepath)\n",
    "utils.save_model(model, 'matches_f85_th81')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "[model, [bm_callback, tb_callback]] = utils.load_model('matches_f85_th81', feature_amount = 85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_watch = x_test\n",
    "y_watch = y_test\n",
    "for i in range(40):\n",
    "    x = np.array(x_watch[i]).reshape((1, -1))\n",
    "    res = model.predict(x)\n",
    "    if y_watch[i] == 1:\n",
    "        print(res, y_watch[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mapping = [tuple(x) for x in utils.read_true_mapping().values]\n",
    "test_lid_rid = list(set(true_mapping) - set(matches))\n",
    "\n",
    "def get_degs(g, uid):\n",
    "    degs = []\n",
    "    for v in g[uid]:\n",
    "        degs.append(g.degree(v))\n",
    "    return degs\n",
    "\n",
    "def double_deg_degs(g, uid):\n",
    "    degs = []\n",
    "    for v in g[uid]:\n",
    "        degs += get_degs(g,v)\n",
    "    return degs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools  as it\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "from random import random\n",
    "reload(utils)\n",
    "\n",
    "def f1_score(pred, ratio):\n",
    "    return 2* (pred * ratio) / (pred + ratio)\n",
    "\n",
    "def res_pred(pred, ratio):\n",
    "    return pred + ratio\n",
    "\n",
    "def neigbor_deg_dist(lg, rg, lid_rid, model, forest):\n",
    "    for lid, rid in lid_rid[140:152]:\n",
    "        try:\n",
    "            degs_l = get_degs(lg, lid)\n",
    "            degs_l2 = double_deg_degs(lg, lid)\n",
    "            lfname = lg.node[lid]['fname']\n",
    "            ldegree = lg.degree(lid)\n",
    "            for vr in [rid] + list(rg[rid].keys())[:4]:\n",
    "                rfname = rg.node[vr]['fname']\n",
    "                ratio = fuzz.token_set_ratio(lfname, rfname) / 100\n",
    "\n",
    "                x = np.array(utils.feature(lg, lid, rg, vr)).reshape((1, -1))\n",
    "                f, axarr = plt.subplots(nrows = 1, ncols=4, figsize=(14,4))\n",
    "                axarr[0].hist(degs_l, bins=30)\n",
    "                axarr[0].set_title('%s \\n %s \\n degree %d | %d' % (lfname, rg.node[vr]['fname'], ldegree, rg.degree(vr)))\n",
    "\n",
    "                degs_r = get_degs(rg, vr)\n",
    "                axarr[1].hist(degs_r, bins=30)\n",
    "                pred = model.predict(x)\n",
    "                pred_forest = forest.predict_proba(x)[0][1]\n",
    "                axarr[1].set_title('model %f \\nfores %f \\n ratio %d' % (pred, pred_forest, ratio))\n",
    "\n",
    "                axarr[2].hist(degs_l2, bins=30)\n",
    "                axarr[2].set_title('avg model ratio: %f' % ((pred + ratio)))\n",
    "\n",
    "                axarr[3].hist(double_deg_degs(rg, vr), bins=30)\n",
    "\n",
    "                plt.show()\n",
    "        except KeyError:\n",
    "            pass\n",
    "        print('###########################################################3')\n",
    "    \n",
    "neigbor_deg_dist(G1, G2, test_lid_rid,model, forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
